{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7280d59-b7c6-4a79-b1db-e1b518f1fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import groq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "import fitz\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from groq import Groq\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6364b3a9-73c8-4fa4-9bf4-635e4e189811",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq.api_key = \"api-key\"\n",
    "\n",
    "# Disable parallelism for HuggingFace tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "groq.api_key = \"api-key\"\n",
    "\n",
    "\n",
    "# Function to get structured data from the image using Groq\n",
    "def get_image_data_from_groq(base64_image):\n",
    "    client = Groq(api_key = \"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\")\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \n",
    "                        f\"\"\"\n",
    "                        ### INSTRUCTION:\n",
    "                        The image is from a page in a document.\n",
    "                        Your job is to extract complete information from the image and return the data in a structured format. It may contain tables, graphs and piecharts or anything.\n",
    "                        ### WITHOUT ANY ADDITION COMMENT, INTRODUCTORY OR CONCLUDING REMARKS (NO PREAMBLE):\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.2-90b-vision-preview\",\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_bytes):\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "def nested_list_to_string(nested_list):\n",
    "    result = []\n",
    "    for sublist in nested_list:\n",
    "        for inner_list in sublist:\n",
    "            # Convert all items to strings, handling None values\n",
    "            formatted_row = ', '.join(str(item) if item is not None else '' for item in inner_list)\n",
    "            result.append(formatted_row)\n",
    "    return '\\n'.join(result)\n",
    "\n",
    "\n",
    "# Function to extract text and images from PDF files\n",
    "def extract_pdf_text_and_images(pdf_paths):\n",
    "    \"\"\"Extract text and image-based content from a list of PDF files using PyMuPDF.\"\"\"\n",
    "    pdf_texts = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                doc = fitz.open(pdf_path)\n",
    "                for count, page in enumerate(pdf.pages, start=1):\n",
    "                    page_number = count\n",
    "                    text = page.extract_text()\n",
    "                    tables = nested_list_to_string(page.extract_tables())\n",
    "\n",
    "                    # Initialize data structure for the current page\n",
    "                    page_data = {\n",
    "                        'page_number': page_number,\n",
    "                        'text': text,\n",
    "                        'tables': \"\",\n",
    "                        'images': tables\n",
    "                    }\n",
    "\n",
    "                    # Extract images from the page using PyMuPDF\n",
    "                    fitz_page = doc.load_page(page_number - 1)\n",
    "                    images = fitz_page.get_images(full=True)\n",
    "                    for img_index, img in enumerate(images):\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image = Image.open(BytesIO(image_bytes))\n",
    "    \n",
    "                        # Check image dimensions\n",
    "                        if image.width < 2 or image.height < 2:\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            # Encode image and get structured data using Groq\n",
    "                            encoded_image = encode_image(image_bytes)\n",
    "                            llm_img_data = get_image_data_from_groq(encoded_image)\n",
    "                            page_data['images'] += f\"Image: {llm_img_data}\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error extracting data from image on page {page_number}, image {img_index + 1}: {e}\")\n",
    "\n",
    "                    \n",
    "                    # Append the page data to the overall pdf_texts list\n",
    "                    pdf_texts.append(page_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_path}: {e}\")\n",
    "    \n",
    "    return pdf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b7fe41-863c-4376-b450-dc9ac8e310d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data extracted\n"
     ]
    }
   ],
   "source": [
    "pdf_paths = [\"example.pdf\"]\n",
    "extracted_data = extract_pdf_text_and_images(pdf_paths)\n",
    "print(\"data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28cc7b2-7a45-47aa-b94e-2e05a4a09b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_number': 19, 'text': 'Example from Psychology\\nWhat do you notice\\nis different in this\\ngraph than the\\nothers reviewed so\\nfar?', 'tables': '', 'images': ''}\n"
     ]
    }
   ],
   "source": [
    "print(extracted_data[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61aee6b-83d5-453a-b603-4bda915313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: doc_1\n",
      "Add of existing embedding ID: doc_1\n",
      "Insert of existing embedding ID: doc_2\n",
      "Add of existing embedding ID: doc_2\n",
      "Insert of existing embedding ID: doc_3\n",
      "Add of existing embedding ID: doc_3\n",
      "Insert of existing embedding ID: doc_4\n",
      "Add of existing embedding ID: doc_4\n",
      "Insert of existing embedding ID: doc_5\n",
      "Add of existing embedding ID: doc_5\n",
      "Insert of existing embedding ID: doc_6\n",
      "Add of existing embedding ID: doc_6\n",
      "Insert of existing embedding ID: doc_7\n",
      "Add of existing embedding ID: doc_7\n",
      "Insert of existing embedding ID: doc_8\n",
      "Add of existing embedding ID: doc_8\n",
      "Insert of existing embedding ID: doc_9\n",
      "Add of existing embedding ID: doc_9\n",
      "Insert of existing embedding ID: doc_10\n",
      "Add of existing embedding ID: doc_10\n",
      "Insert of existing embedding ID: doc_11\n",
      "Add of existing embedding ID: doc_11\n",
      "Insert of existing embedding ID: doc_12\n",
      "Add of existing embedding ID: doc_12\n",
      "Insert of existing embedding ID: doc_13\n",
      "Add of existing embedding ID: doc_13\n",
      "Insert of existing embedding ID: doc_14\n",
      "Add of existing embedding ID: doc_14\n",
      "Insert of existing embedding ID: doc_15\n",
      "Add of existing embedding ID: doc_15\n",
      "Insert of existing embedding ID: doc_16\n",
      "Add of existing embedding ID: doc_16\n",
      "Insert of existing embedding ID: doc_17\n",
      "Add of existing embedding ID: doc_17\n",
      "Insert of existing embedding ID: doc_18\n",
      "Add of existing embedding ID: doc_18\n",
      "Insert of existing embedding ID: doc_19\n",
      "Add of existing embedding ID: doc_19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully added to ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"document\")\n",
    "\n",
    "# Add data to the collection\n",
    "for page in extracted_data:\n",
    "    data = f\"Page_Number: PAGE {page['page_number']}\\n\\n Page_Image_Description: {page['images']}\\n\\n TABLE: Table {page['tables']}\\n\\n Page_Text: {page['text']}\\n\\n\"\n",
    "\n",
    "    collection.add(\n",
    "        documents=[data],\n",
    "        metadatas={\"page_number\": f\"page {page['page_number']}\"},\n",
    "        ids=[f\"doc_{page['page_number']}\"]\n",
    "    )\n",
    "print(\"Data successfully added to ChromaDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f36c39-1a4e-47c9-b6ae-fe1fdf3e988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc_14', 'doc_1', 'doc_3', 'doc_5', 'doc_4', 'doc_15', 'doc_6']], 'embeddings': None, 'documents': [['Page_Number: PAGE 14\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: • If given a table of data, we should be able to plot it. Below is\\nsome sample data; plot the data with x on the x-axis and y on the\\ny-axis.\\nx y\\n0 0\\n1 3\\n2 6\\n3 9\\n4 12\\n5 15\\n6 18\\n7 21\\n8 24\\n\\n', 'Page_Number: PAGE 1\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: Tables, Charts, and\\nGraphs\\nwith Examples from History, Economics,\\nEducation, Psychology, Urban Affairs and\\nEveryday Life\\nREVISED: MICHAEL LOLKUS 2018\\n\\n', 'Page_Number: PAGE 3\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: Tables, Charts, and\\nGraphs Basics\\n\\n', 'Page_Number: PAGE 5\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: Types of Visual\\nRepresentations of Data\\n\\n', 'Page_Number: PAGE 4\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: \\uf075 We use charts and graphs to visualize data.\\n\\uf075 This data can either be generated data, data gathered from\\nan experiment, or data collected from some source.\\n\\uf075 A picture tells a thousand words so it is not a surprise that\\nmany people use charts and graphs when explaining data.\\n\\n', 'Page_Number: PAGE 15\\n\\n Page_Image_Description: \\n\\n TABLE: Table \\n\\n Page_Text: • Below is a plot of the data on the table from the previous\\nslide. Notice that this plot is a straight line meaning that a\\nlinear equation must have generated this data.\\n• What if the data is not generated by a linear equation? We can\\nfit the data using a linear regression and use that line as an\\napproximation to the data. Regressions are beyond the scope of\\nthis workshop.\\n30\\n25\\n20\\n15\\n10\\n5\\n0\\n0 1 2 3 4 5 6 7 8\\n\\n', 'Page_Number: PAGE 6\\n\\n Page_Image_Description: Year, 2010, 2011, 2012, 2013, 2014, 2015\\nAll Industries, 26093515, 27535971, 28663246, 29601191, 30895407, 31397023\\nManufacturing, 4992521, 5581942, 5841608, 5953299, 6047477, 5829554\\nFinance,\\nInsurance, Real\\nEstate, Rental,\\nLeasing, 4522451, 4618678, 4797313, 5031881, 5339678, 5597018\\nArts,\\nEntertainment,\\nRecreation,\\nAccommodation,\\nand Food Service, 964032, 1015238, 1076249, 1120496, 1189646, 1283813\\nOther, 15614511, 16320113, 16948076, 17495515, 18318606, 18686638\\n\\n TABLE: Table \\n\\n Page_Text: Table of Yearly U.S. GDP by\\nIndustry (in millions of dollars)\\nSource: U.S. Bureau of Labor Statistics\\nYear 2010 2011 2012 2013 2014 2015\\nAll Industries 26093515 27535971 28663246 29601191 30895407 31397023\\nManufacturing 4992521 5581942 5841608 5953299 6047477 5829554\\nFinance,\\nInsurance, Real\\n4522451 4618678 4797313 5031881 5339678 5597018\\nEstate, Rental,\\nLeasing\\nArts,\\nEntertainment,\\nRecreation, 964032 1015238 1076249 1120496 1189646 1283813\\nAccommodation,\\nand Food Service\\nOther 15614511 16320113 16948076 17495515 18318606 18686638\\n\\n']], 'uris': None, 'data': None, 'metadatas': [[{'page_number': 'page 14'}, {'page_number': 'page 1'}, {'page_number': 'page 3'}, {'page_number': 'page 5'}, {'page_number': 'page 4'}, {'page_number': 'page 15'}, {'page_number': 'page 6'}]], 'distances': [[1.1181970834732056, 1.1788122653961182, 1.1870698928833008, 1.212778091430664, 1.2868924140930176, 1.3332319259643555, 1.3714522123336792]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"From page 6 get the tabular data\"], # Chroma will embed this for you\n",
    "    n_results=7 # how many results to return\n",
    ")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8d75bd-5fdf-41e7-a5b5-0b87efa7e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From page 6, the tabular data is:\n",
      "\n",
      "| Year | All Industries | Manufacturing | Finance, Insurance, Real Estate, Rental, Leasing | Arts, Entertainment, Recreation, Accommodation, and Food Service | Other |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| 2010 | 26093515 | 4992521 | 4522451 | 964032 | 15614511 |\n",
      "| 2011 | 27535971 | 5581942 | 4618678 | 1015238 | 16320113 |\n",
      "| 2012 | 28663246 | 5841608 | 4797313 | 1076249 | 16948076 |\n",
      "| 2013 | 29601191 | 5953299 | 5031881 | 1120496 | 17495515 |\n",
      "| 2014 | 30895407 | 6047477 | 5339678 | 1189646 | 18318606 |\n",
      "| 2015 | 31397023 | 5829554 | 5597018 | 1283813 | 18686638 |\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=groq.api_key)\n",
    "\n",
    "query = \"From page 6 get the tabular data\"\n",
    "\n",
    "page = collection.query(\n",
    "    query_texts=[query], # Chroma will embed this for you\n",
    "    n_results=7 # how many results to return\n",
    ")['documents'][0]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        ### PAGE CONTENT:\n",
    "        {page}\n",
    "        ### INSTRUCTION:\n",
    "        You are an assistant tasked with providing relevant information from the above page content based on the following query: {query}.\n",
    "        ### (NO PREAMBLE):\n",
    "\n",
    "        \"\"\"\n",
    "    )\n",
    "query_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "query_result = query_chain.invoke({\"page\":page, \"query\": query})\n",
    "print(query_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
